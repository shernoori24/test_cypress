# app/models/enhanced_data_preprocessor.py
# Enhanced preprocessing with validation and realistic demonstration data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import logging
from pathlib import Path
from datetime import datetime, timedelta
import warnings
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import IsolationForest
import os

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

class EnhancedDataPreprocessor:
    """
    Enhanced preprocessing with validation, outlier detection, and feature engineering
    """
    
    def __init__(self, data_folder_path):
        self.data_folder = Path(data_folder_path)
        self.file_path = self.data_folder / "inscription.xlsx"
        self.df_raw = None
        self.df_clean = None
        self.df_features = None
        self.encoders = {}
        self.scalers = {}
        self.validation_results = {}
        
        # Create visualization folder
        self.viz_folder = Path("app/static/img/ml_viz")
        self.viz_folder.mkdir(parents=True, exist_ok=True)
        
    def validate_input_file(self):
        """
        Validate input Excel file structure and quality
        
        Returns:
            dict: Validation results with recommendations
        """
        logger.info("=== VALIDATION DU FICHIER D'ENTR√âE ===")
        
        validation_results = {
            'file_exists': False,
            'data_quality': {},
            'temporal_coverage': {},
            'issues': [],
            'recommendations': []
        }
        
        # Check file existence
        if not self.file_path.exists():
            validation_results['issues'].append(f"File not found: {self.file_path}")
            logger.error(f"‚ùå File not found: {self.file_path}")
            return validation_results
        
        validation_results['file_exists'] = True
        
        try:
            # Load and analyze data
            df = pd.read_excel(self.file_path)
            
            # Basic structure validation
            validation_results['data_quality'] = {
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'missing_percentage': (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
            }
            
            # Date column validation
            if 'Date inscription' in df.columns:
                dates = pd.to_datetime(df['Date inscription'], errors='coerce').dropna()
                if len(dates) > 0:
                    validation_results['temporal_coverage'] = {
                        'valid_dates': len(dates),
                        'date_range_years': (dates.max() - dates.min()).days / 365.25,
                        'min_date': dates.min(),
                        'max_date': dates.max(),
                        'entries_per_year': len(dates) / ((dates.max() - dates.min()).days / 365.25)
                    }
                    
                    # Temporal quality checks
                    yearly_counts = dates.groupby(dates.dt.year).size()
                    if yearly_counts.min() < 10:
                        validation_results['issues'].append("Some years have very few enrollments (<10)")
                    if yearly_counts.std() > yearly_counts.mean():
                        validation_results['issues'].append("High variance in yearly enrollments")
            
            # Missing data analysis
            missing_cols = df.isnull().sum()
            critical_missing = missing_cols[missing_cols > len(df) * 0.8]
            if len(critical_missing) > 0:
                validation_results['issues'].append(f"High missing data: {list(critical_missing.index)}")
            
            # Generate recommendations
            if validation_results['data_quality']['missing_percentage'] > 30:
                validation_results['recommendations'].append("Consider data cleaning strategies")
            if validation_results['temporal_coverage'].get('entries_per_year', 0) < 50:
                validation_results['recommendations'].append("Low data density - consider aggregating by quarters")
            
            logger.info(f"‚úÖ Validation completed - {len(validation_results['issues'])} issues found")
            
        except Exception as e:
            validation_results['issues'].append(f"Error reading file: {e}")
            logger.error(f"‚ùå Validation error: {e}")
        
        self.validation_results = validation_results
        return validation_results
    
    def create_demonstration_dataset(self):
        """
        Create realistic demonstration dataset with proper temporal patterns
        """
        logger.info("=== CR√âATION DATASET D√âMONSTRATION ===")
        
        # Generate realistic enrollment data (2015-2025)
        np.random.seed(42)
        
        start_date = datetime(2015, 1, 1)
        end_date = datetime(2025, 12, 31)
        
        # Create monthly data points
        dates = []
        enrollments = []
        
        current_date = start_date
        base_trend = 50  # Base enrollments per year
        
        while current_date <= end_date:
            # Seasonal pattern (higher in Sept-Oct, Jan-Feb, lower in Jul-Aug)
            month = current_date.month
            seasonal_factor = {
                1: 1.3, 2: 1.2, 3: 1.0, 4: 0.9, 5: 0.8, 6: 0.7,
                7: 0.6, 8: 0.5, 9: 1.5, 10: 1.4, 11: 1.1, 12: 0.9
            }[month]
            
            # Growth trend (5% per year)
            year_growth = 1.05 ** (current_date.year - 2015)
            
            # Random variation
            noise = np.random.normal(1.0, 0.3)
            
            # Calculate monthly enrollments
            monthly_base = (base_trend * year_growth * seasonal_factor * noise) / 12
            monthly_enrollments = max(1, int(np.round(monthly_base)))
            
            dates.append(current_date)
            enrollments.append(monthly_enrollments)
            
            # Move to next month
            if current_date.month == 12:
                current_date = current_date.replace(year=current_date.year + 1, month=1)
            else:
                current_date = current_date.replace(month=current_date.month + 1)
        
        # Create demonstration dataframe
        demo_df = pd.DataFrame({
            'date': dates,
            'enrollments': enrollments
        })
        
        # Add additional features for demonstration
        demo_df['year'] = demo_df['date'].dt.year
        demo_df['month'] = demo_df['date'].dt.month
        demo_df['quarter'] = demo_df['date'].dt.quarter
        demo_df['is_academic_start'] = demo_df['month'].isin([9, 10, 1, 2]).astype(int)
        demo_df['is_summer'] = demo_df['month'].isin([6, 7, 8]).astype(int)
        
        # Save demonstration dataset
        demo_path = self.data_folder / "demonstration_enrollments.xlsx"
        demo_df.to_excel(demo_path, index=False)
        
        logger.info(f"‚úÖ Demonstration dataset created: {len(demo_df)} months from {dates[0]} to {dates[-1]}")
        logger.info(f"üìÅ Saved to: {demo_path}")
        
        return demo_df
    
    def load_data_with_validation(self):
        """
        Load data with validation and fallback to demonstration dataset
        """
        logger.info("=== CHARGEMENT DONN√âES AVEC VALIDATION ===")
        
        # First, validate the input file
        validation = self.validate_input_file()
        
        if not validation['file_exists'] or len(validation['issues']) > 3:
            logger.warning("‚ö†Ô∏è Input file issues detected, using demonstration dataset")
            demo_df = self.create_demonstration_dataset()
            
            # Convert demo format to match expected format
            self.df_raw = pd.DataFrame({
                'Date inscription': demo_df['date'],
                'Sexe': np.random.choice(['M', 'F'], len(demo_df), p=[0.6, 0.4]),
                'Nationalit√©': np.random.choice(['F', 'E', 'A'], len(demo_df), p=[0.2, 0.7, 0.1]),
                'Ville': np.random.choice(['CHARLEVILLE-MEZIERES', 'SEDAN', 'REVIN'], len(demo_df), p=[0.7, 0.2, 0.1]),
                'Prioritaire/Veille': np.random.choice(['Prioritaire', 'NP'], len(demo_df), p=[0.5, 0.5])
            })
        else:
            logger.info("‚úÖ Loading original dataset")
            self.df_raw = pd.read_excel(self.file_path)
        
        logger.info(f"üìä Dataset loaded: {self.df_raw.shape[0]} rows, {self.df_raw.shape[1]} columns")
        return self.df_raw
    
    def detect_outliers(self, df, features):
        """
        Detect outliers in temporal data using Isolation Forest
        """
        logger.info("=== D√âTECTION D'OUTLIERS ===")
        
        if len(features) == 0 or len(df) < 10:
            return df.index.tolist(), []
        
        # Prepare features for outlier detection
        X = df[features].fillna(df[features].median())
        
        # Use Isolation Forest for outlier detection
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        outlier_pred = iso_forest.fit_predict(X)
        
        normal_indices = df.index[outlier_pred == 1].tolist()
        outlier_indices = df.index[outlier_pred == -1].tolist()
        
        logger.info(f"üìà Outliers detected: {len(outlier_indices)} / {len(df)} ({len(outlier_indices)/len(df)*100:.1f}%)")
        
        return normal_indices, outlier_indices
    
    def create_lag_features(self, df, target_col, lags=[1, 3, 6, 12]):
        """
        Create lag features for time series prediction
        """
        logger.info("=== CR√âATION LAG FEATURES ===")
        
        df_lag = df.copy()
        
        for lag in lags:
            lag_col = f"{target_col}_lag_{lag}"
            df_lag[lag_col] = df_lag[target_col].shift(lag)
            
        # Rolling window features
        for window in [3, 6, 12]:
            if len(df_lag) > window:
                df_lag[f"{target_col}_rolling_mean_{window}"] = df_lag[target_col].rolling(window=window).mean()
                df_lag[f"{target_col}_rolling_std_{window}"] = df_lag[target_col].rolling(window=window).std()
        
        logger.info(f"‚úÖ Lag features created for lags: {lags}")
        return df_lag
    
    def advanced_temporal_preprocessing(self):
        """
        Advanced temporal preprocessing with lag features and outlier detection
        """
        logger.info("=== PREPROCESSING TEMPOREL AVANC√â ===")
        
        if self.df_raw is None:
            raise ValueError("Data must be loaded first")
        
        # Convert to temporal aggregations
        if 'Date inscription' in self.df_raw.columns:
            dates = pd.to_datetime(self.df_raw['Date inscription'], errors='coerce').dropna()
            
            # Monthly aggregations with proper column handling
            year_month_groups = dates.groupby([dates.dt.year, dates.dt.month]).size()
            
            # Create dataframe from groupby result
            monthly_data = year_month_groups.reset_index(name='enrollments')
            monthly_data.columns = ['year', 'month', 'enrollments']
            
            # Create proper date column
            monthly_data['date'] = pd.to_datetime(monthly_data[['year', 'month']].assign(day=1))
            monthly_data = monthly_data.sort_values('date').reset_index(drop=True)
            
            # Create lag features
            monthly_data = self.create_lag_features(monthly_data, 'enrollments')
            
            # Detect outliers
            feature_cols = [col for col in monthly_data.columns if 'enrollments' in col and 'lag' not in col]
            normal_idx, outlier_idx = self.detect_outliers(monthly_data, feature_cols)
            
            # Mark outliers
            monthly_data['is_outlier'] = False
            monthly_data.loc[outlier_idx, 'is_outlier'] = True
            
            self.df_features = monthly_data
            
            logger.info(f"‚úÖ Advanced temporal preprocessing completed")
            logger.info(f"üìä Features dataset: {len(self.df_features)} months")
            
        return self.df_features
    
    def prepare_ml_datasets(self):
        """
        Prepare datasets for machine learning with advanced features
        """
        logger.info("=== PR√âPARATION DATASETS ML ===")
        
        if self.df_features is None:
            self.advanced_temporal_preprocessing()
        
        # Remove rows with too many NaN values (from lag features)
        df_ml = self.df_features.dropna(subset=['enrollments']).copy()
        
        # Features for prediction
        feature_cols = [col for col in df_ml.columns if 
                       col not in ['date', 'enrollments', 'is_outlier'] and 
                       not df_ml[col].isnull().all()]
        
        # Clean dataset (remove outliers for training)
        df_clean = df_ml[~df_ml['is_outlier']].copy()
        
        X = df_clean[feature_cols].ffill().fillna(0)
        y = df_clean['enrollments']
        dates = df_clean['date']
        
        logger.info(f"‚úÖ ML dataset prepared: {len(X)} samples, {len(feature_cols)} features")
        
        return {
            'X': X,
            'y': y,
            'dates': dates,
            'feature_names': feature_cols,
            'full_data': df_ml
        }
    
    def get_preprocessing_summary(self):
        """
        Get comprehensive preprocessing summary
        """
        summary = {
            'validation_results': self.validation_results,
            'data_shape': self.df_raw.shape if self.df_raw is not None else None,
            'features_shape': self.df_features.shape if self.df_features is not None else None,
            'processing_steps': [
                'Input validation',
                'Data loading with fallback',
                'Temporal aggregation',
                'Lag feature creation',
                'Outlier detection',
                'ML dataset preparation'
            ]
        }
        
        return summary